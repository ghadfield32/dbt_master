{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Taxi Analytics with dbt + DuckDB: A Complete Walkthrough\n",
    "\n",
    "This notebook walks through the **design decisions and thought process** behind building a production-grade dbt project from scratch. We'll connect directly to the DuckDB database that dbt built and explore every layer of the pipeline.\n",
    "\n",
    "**What you'll learn:**\n",
    "1. Why we chose DuckDB + dbt for local analytics\n",
    "2. How raw NYC taxi data flows through staging → intermediate → marts\n",
    "3. The thinking behind dimensional modeling (facts vs. dimensions)\n",
    "4. Testing strategies for real-world messy data\n",
    "5. Advanced features: incremental models, contracts, Python models, parquet exports\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Make sure you've run the full build first:\n",
    "```bash\n",
    "uv sync\n",
    "uv run python scripts/download_data.py\n",
    "uv run python scripts/setup_project.py\n",
    "cd nyc_taxi_dbt && uv run dbt build --full-refresh --profiles-dir .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Connect to Our DuckDB Warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "from pathlib import Path\n",
    "\n",
    "# Connect to the dbt-managed database\n",
    "DB_PATH = Path(\"dev.duckdb\")\n",
    "assert DB_PATH.exists(), \"Run 'make build' first to create the database\"\n",
    "\n",
    "con = duckdb.connect(str(DB_PATH), read_only=True)\n",
    "\n",
    "# Helper to run queries and display results\n",
    "def q(sql):\n",
    "    \"\"\"Run a SQL query and return a pandas DataFrame.\"\"\"\n",
    "    return con.execute(sql).fetchdf()\n",
    "\n",
    "# What schemas did dbt create?\n",
    "q(\"\"\"\n",
    "    SELECT schema_name, count(*) as tables\n",
    "    FROM information_schema.tables\n",
    "    WHERE schema_name NOT IN ('information_schema', 'pg_catalog')\n",
    "    GROUP BY schema_name\n",
    "    ORDER BY schema_name\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dbt organized our data into **separate schemas by layer**:\n",
    "- `main_raw` -- Seed data (CSV reference tables loaded by `dbt seed`)\n",
    "- `main_staging` -- Cleaned, renamed versions of raw data\n",
    "- `main_intermediate` -- Calculated metrics and aggregations\n",
    "- `main_marts` -- Business-ready facts, dimensions, and analytics\n",
    "- `snapshots` -- SCD Type 2 historical tracking\n",
    "\n",
    "This separation is a **dbt best practice**. Each layer has a single responsibility, making the pipeline easy to debug and maintain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: The Raw Data -- What Are We Working With?\n",
    "\n",
    "### Thought Process\n",
    "\n",
    "Before writing any dbt models, we need to understand our data. The NYC Taxi & Limousine Commission publishes trip records monthly as parquet files. January 2024 has ~3 million yellow taxi trips.\n",
    "\n",
    "**Why parquet?** Column-oriented, compressed, includes schema metadata. DuckDB reads it natively without importing -- perfect for analytics.\n",
    "\n",
    "**Why DuckDB?** Zero infrastructure. No Docker, no server, no credentials. Just a single file. It handles 3M rows in under 2 seconds on a laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's peek at the raw parquet file directly\n",
    "raw_stats = q(\"\"\"\n",
    "    SELECT\n",
    "        count(*) as total_rows,\n",
    "        min(tpep_pickup_datetime) as earliest_pickup,\n",
    "        max(tpep_pickup_datetime) as latest_pickup,\n",
    "        count(DISTINCT PULocationID) as unique_pickup_zones,\n",
    "        count(DISTINCT DOLocationID) as unique_dropoff_zones\n",
    "    FROM read_parquet('data/yellow_tripdata_2024-01.parquet')\n",
    "\"\"\")\n",
    "print(f\"Raw parquet: {raw_stats['total_rows'][0]:,} rows\")\n",
    "print(f\"Date range: {raw_stats['earliest_pickup'][0]} to {raw_stats['latest_pickup'][0]}\")\n",
    "print(f\"Unique zones: {raw_stats['unique_pickup_zones'][0]} pickup, {raw_stats['unique_dropoff_zones'][0]} dropoff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does the raw data look like? (column names are messy!)\n",
    "q(\"SELECT * FROM read_parquet('data/yellow_tripdata_2024-01.parquet') LIMIT 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problems we can already see:**\n",
    "- Column names are inconsistent: `VendorID`, `PULocationID`, `tpep_pickup_datetime`, `Airport_fee` (mixed case)\n",
    "- Location IDs are just numbers -- meaningless without a lookup table\n",
    "- Payment types are coded (1, 2, 3...) -- not human-readable\n",
    "- Financial amounts need rounding to 2 decimal places\n",
    "- There might be null values, negative fares, or out-of-range dates\n",
    "\n",
    "This is exactly what the **staging layer** is for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Staging -- Clean Once, Use Everywhere\n",
    "\n",
    "### Thought Process\n",
    "\n",
    "The staging layer is a **contract between raw data and the rest of the pipeline**. Its jobs:\n",
    "1. **Rename** columns to consistent `snake_case`\n",
    "2. **Cast** types explicitly (don't trust implicit casting)\n",
    "3. **Filter** obviously bad data (nulls in required fields, negative fares)\n",
    "4. **Generate** surrogate keys for deduplication\n",
    "\n",
    "The rule: **one staging model per source**. Every downstream model references staging, never raw data directly.\n",
    "\n",
    "Here's what `stg_yellow_trips` does:\n",
    "\n",
    "```sql\n",
    "-- Key parts of stg_yellow_trips.sql\n",
    "select\n",
    "    -- Surrogate key for deduplication\n",
    "    {{ dbt_utils.generate_surrogate_key([\n",
    "        'VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime',\n",
    "        'PULocationID', 'DOLocationID', 'fare_amount', 'total_amount'\n",
    "    ]) }} as trip_id,\n",
    "\n",
    "    -- Consistent naming + explicit casting\n",
    "    cast(\"VendorID\" as integer) as vendor_id,\n",
    "    cast(\"PULocationID\" as integer) as pickup_location_id,\n",
    "    round(cast(fare_amount as decimal(10, 2)), 2) as fare_amount,\n",
    "    ...\n",
    "from source\n",
    "where tpep_pickup_datetime is not null    -- filter bad data\n",
    "  and fare_amount >= 0                     -- no negative fares\n",
    "  and cast(tpep_pickup_datetime as date) >= date '2024-01-01'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many rows survived staging filters?\n",
    "raw_count = q(\"SELECT count(*) as n FROM read_parquet('data/yellow_tripdata_2024-01.parquet')\")['n'][0]\n",
    "staged_count = q(\"SELECT count(*) as n FROM main_staging.stg_yellow_trips\")['n'][0]\n",
    "filtered = raw_count - staged_count\n",
    "\n",
    "print(f\"Raw rows:     {raw_count:>12,}\")\n",
    "print(f\"After staging:{staged_count:>12,}\")\n",
    "print(f\"Filtered out: {filtered:>12,} ({filtered/raw_count*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean column names -- compare raw vs staged\n",
    "print(\"=== Raw Columns ===\")\n",
    "raw_cols = q(\"SELECT column_name FROM (DESCRIBE SELECT * FROM read_parquet('data/yellow_tripdata_2024-01.parquet'))\")\n",
    "print(\", \".join(raw_cols['column_name'].tolist()))\n",
    "\n",
    "print(\"\\n=== Staged Columns ===\")\n",
    "staged_cols = q(\"SELECT column_name FROM information_schema.columns WHERE table_schema='main_staging' AND table_name='stg_yellow_trips' ORDER BY ordinal_position\")\n",
    "print(\", \".join(staged_cols['column_name'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed tables -- small reference CSVs loaded by dbt seed\n",
    "print(\"=== Payment Types ===\")\n",
    "display(q(\"SELECT * FROM main_raw.payment_type_lookup ORDER BY payment_type_id\"))\n",
    "\n",
    "print(\"\\n=== Rate Codes ===\")\n",
    "display(q(\"SELECT * FROM main_raw.rate_code_lookup ORDER BY rate_code_id\"))\n",
    "\n",
    "print(\"\\n=== Taxi Zones (sample) ===\")\n",
    "display(q(\"SELECT * FROM main_raw.taxi_zone_lookup ORDER BY LocationID LIMIT 10\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Decision: Why Include fare_amount + total_amount in the Surrogate Key?\n",
    "\n",
    "NYC taxi data has **adjustment records** -- the same vendor, same timestamps, same locations, but different fare amounts (one original, one corrected). Without including financial columns in the key, these would collide and we'd lose data. This is a real-world data quality issue you only discover by testing.\n",
    "\n",
    "### Data Quality Findings\n",
    "- ~37K rows with negative fares (adjustment records) -- filtered out\n",
    "- ~18 rows with dates outside January 2024 -- filtered out\n",
    "- ~140K rows with null passenger_count -- kept (non-critical field)\n",
    "- Location IDs 264/265 exist in trips but not in zone lookup -- handled with `severity: warn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Intermediate -- Calculate Metrics\n",
    "\n",
    "### Thought Process\n",
    "\n",
    "The intermediate layer does **transformations that multiple downstream models need**. Instead of calculating trip duration in 5 different places, we calculate it once here.\n",
    "\n",
    "`int_trip_metrics` adds:\n",
    "- `trip_duration_minutes` -- using a custom macro: `{{ duration_minutes('pickup_datetime', 'dropoff_datetime') }}`\n",
    "- `avg_speed_mph` -- distance / (duration / 60)\n",
    "- `cost_per_mile` -- fare / distance\n",
    "- `tip_percentage` -- tip / fare * 100\n",
    "- Time dimensions: `pickup_date`, `pickup_hour`, `pickup_day_of_week`, `is_weekend`\n",
    "\n",
    "It also **filters impossible trips**: duration < 1 minute, > 12 hours, or speed > 100 mph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row counts through the pipeline\n",
    "counts = q(\"\"\"\n",
    "    SELECT\n",
    "        (SELECT count(*) FROM main_staging.stg_yellow_trips) as staging,\n",
    "        (SELECT count(*) FROM main_intermediate.int_trip_metrics) as intermediate,\n",
    "        (SELECT count(*) FROM main_marts.fct_trips) as marts\n",
    "\"\"\")\n",
    "\n",
    "stg = counts['staging'][0]\n",
    "intm = counts['intermediate'][0]\n",
    "mart = counts['marts'][0]\n",
    "\n",
    "print(f\"Staging:      {stg:>12,} rows\")\n",
    "print(f\"Intermediate: {intm:>12,} rows  (filtered {stg - intm:,} impossible trips)\")\n",
    "print(f\"Marts:        {mart:>12,} rows  (1:1 with intermediate -- joins don't add/remove rows)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does the enriched data look like?\n",
    "q(\"\"\"\n",
    "    SELECT\n",
    "        trip_id,\n",
    "        pickup_date,\n",
    "        pickup_hour,\n",
    "        pickup_day_of_week,\n",
    "        is_weekend,\n",
    "        trip_duration_minutes,\n",
    "        trip_distance_miles,\n",
    "        round(avg_speed_mph, 1) as avg_speed_mph,\n",
    "        fare_amount,\n",
    "        tip_amount,\n",
    "        round(tip_percentage, 1) as tip_pct,\n",
    "        round(cost_per_mile, 2) as cost_per_mile\n",
    "    FROM main_intermediate.int_trip_metrics\n",
    "    WHERE trip_distance_miles > 0\n",
    "    LIMIT 10\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of calculated metrics\n",
    "q(\"\"\"\n",
    "    SELECT\n",
    "        round(avg(trip_duration_minutes), 1) as avg_duration_min,\n",
    "        round(median(trip_duration_minutes), 1) as median_duration_min,\n",
    "        round(avg(avg_speed_mph), 1) as avg_speed,\n",
    "        round(avg(tip_percentage), 1) as avg_tip_pct,\n",
    "        round(avg(cost_per_mile), 2) as avg_cost_per_mile,\n",
    "        count(CASE WHEN is_weekend THEN 1 END) as weekend_trips,\n",
    "        count(CASE WHEN NOT is_weekend THEN 1 END) as weekday_trips\n",
    "    FROM main_intermediate.int_trip_metrics\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Pre-Aggregation Models\n",
    "\n",
    "Two more intermediate models aggregate trip-level data:\n",
    "\n",
    "- **`int_daily_summary`** -- One row per day (31 rows for January). Total trips, revenue, averages.\n",
    "- **`int_hourly_patterns`** -- One row per date + hour (~744 rows). For understanding demand curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily summary -- what does January look like?\n",
    "q(\"\"\"\n",
    "    SELECT\n",
    "        pickup_date,\n",
    "        pickup_day_of_week,\n",
    "        is_weekend,\n",
    "        total_trips,\n",
    "        round(total_revenue, 0) as total_revenue,\n",
    "        round(avg_trip_distance, 1) as avg_dist,\n",
    "        round(avg_tip_percentage, 1) as avg_tip_pct\n",
    "    FROM main_intermediate.int_daily_summary\n",
    "    ORDER BY pickup_date\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Marts -- The Business Layer\n",
    "\n",
    "### Thought Process: Dimensional Modeling\n",
    "\n",
    "The marts layer follows **dimensional modeling** (Kimball methodology):\n",
    "\n",
    "- **Fact tables** contain events/transactions with numeric measures (fares, distances, durations)\n",
    "- **Dimension tables** contain descriptive attributes for filtering and grouping (locations, dates, payment types)\n",
    "\n",
    "This design answers: *\"Who took a trip, where, when, how did they pay, and how much did it cost?\"*\n",
    "\n",
    "```\n",
    "                    dim_locations\n",
    "                         |\n",
    "int_trip_metrics --> fct_trips <-- dim_dates\n",
    "                         |\n",
    "                    dim_payment_types\n",
    "```\n",
    "\n",
    "### Why Not Just One Big Table?\n",
    "\n",
    "You could, and for a small project it works fine. But dimensional modeling:\n",
    "1. **Reduces redundancy** -- zone names stored once in `dim_locations`, not 2.9M times in the fact table\n",
    "2. **Enables flexible analysis** -- join any dimension to the fact table\n",
    "3. **Scales** -- when you add a new dimension (e.g., `dim_weather`), you only add one join\n",
    "4. **Is the industry standard** -- every analytics engineer is expected to know this pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the dimension tables\n",
    "print(f\"dim_locations:      {q('SELECT count(*) as n FROM main_marts.dim_locations')['n'][0]} zones\")\n",
    "print(f\"dim_dates:          {q('SELECT count(*) as n FROM main_marts.dim_dates')['n'][0]} days\")\n",
    "print(f\"dim_payment_types:  {q('SELECT count(*) as n FROM main_marts.dim_payment_types')['n'][0]} types\")\n",
    "\n",
    "print(\"\\n=== dim_dates (sample) ===\")\n",
    "display(q(\"SELECT * FROM main_marts.dim_dates LIMIT 7\"))\n",
    "\n",
    "print(\"\\n=== dim_locations (top boroughs) ===\")\n",
    "display(q(\"\"\"\n",
    "    SELECT borough, count(*) as zones\n",
    "    FROM main_marts.dim_locations\n",
    "    GROUP BY borough ORDER BY zones DESC\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The fact table -- trips enriched with location names\n",
    "q(\"\"\"\n",
    "    SELECT\n",
    "        trip_id,\n",
    "        pickup_datetime,\n",
    "        pickup_borough,\n",
    "        pickup_zone,\n",
    "        dropoff_borough,\n",
    "        dropoff_zone,\n",
    "        trip_duration_minutes,\n",
    "        trip_distance_miles,\n",
    "        fare_amount,\n",
    "        tip_amount,\n",
    "        total_amount\n",
    "    FROM main_marts.fct_trips\n",
    "    WHERE pickup_borough = 'Manhattan'\n",
    "    LIMIT 10\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytics Marts -- Pre-Aggregated for Speed\n",
    "\n",
    "While the fact table supports any ad-hoc query, common business questions deserve **pre-computed answers**:\n",
    "\n",
    "| Mart | Question It Answers |\n",
    "|------|---------------------|\n",
    "| `mart_daily_revenue` | How much revenue per day? What's the trend? |\n",
    "| `mart_location_performance` | Which zones are most profitable? |\n",
    "| `mart_hourly_demand` | When do people take taxis? Weekday vs weekend? |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily revenue trend\n",
    "daily = q(\"\"\"\n",
    "    SELECT\n",
    "        date_key,\n",
    "        day_of_week_name,\n",
    "        is_weekend,\n",
    "        total_trips,\n",
    "        round(total_revenue, 0) as revenue,\n",
    "        round(cumulative_revenue, 0) as cumulative,\n",
    "        round(revenue_change_vs_prior_day, 0) as day_over_day\n",
    "    FROM main_marts.mart_daily_revenue\n",
    "    ORDER BY date_key\n",
    "\"\"\")\n",
    "daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 15 pickup zones by total pickups\n",
    "q(\"\"\"\n",
    "    SELECT\n",
    "        pickup_borough,\n",
    "        pickup_zone,\n",
    "        total_pickups,\n",
    "        round(total_revenue, 0) as total_revenue,\n",
    "        round(avg_revenue_per_trip, 2) as avg_revenue,\n",
    "        round(avg_tip_pct, 1) as avg_tip_pct,\n",
    "        most_common_dropoff_zone,\n",
    "        peak_pickup_hour\n",
    "    FROM main_marts.mart_location_performance\n",
    "    ORDER BY total_pickups DESC\n",
    "    LIMIT 15\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hourly demand: weekday vs weekend\n",
    "q(\"\"\"\n",
    "    SELECT\n",
    "        pickup_hour,\n",
    "        CASE WHEN is_weekend THEN 'Weekend' ELSE 'Weekday' END as day_type,\n",
    "        round(avg_trips_per_period, 0) as avg_trips,\n",
    "        round(avg_revenue_per_period, 0) as avg_revenue,\n",
    "        round(avg_distance, 1) as avg_distance,\n",
    "        round(avg_duration_min, 1) as avg_duration\n",
    "    FROM main_marts.mart_hourly_demand\n",
    "    ORDER BY is_weekend, pickup_hour\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Testing -- How We Trust the Data\n",
    "\n",
    "### Thought Process\n",
    "\n",
    "Real-world data is messy. Tests are how we define and enforce our expectations. dbt supports:\n",
    "\n",
    "| Test Type | What It Does | Example |\n",
    "|-----------|-------------|----------|\n",
    "| **Generic** (YAML) | Reusable checks | `not_null`, `unique`, `accepted_values` |\n",
    "| **Singular** (SQL) | Custom queries that return failing rows | `assert_trip_duration_positive.sql` |\n",
    "| **Unit** (YAML) | Test transformations with mock data | Verify column renaming, calculation logic |\n",
    "| **Custom generic** (macro) | Your own reusable test | `test_positive_value` |\n",
    "\n",
    "Our project has **91 tests**: 84 data tests + 7 unit tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's verify key data quality expectations manually\n",
    "print(\"=== Data Quality Checks ===\")\n",
    "\n",
    "# 1. No null trip IDs\n",
    "null_ids = q(\"SELECT count(*) as n FROM main_staging.stg_yellow_trips WHERE trip_id IS NULL\")['n'][0]\n",
    "print(f\"Null trip_ids in staging: {null_ids} (expected: 0)\")\n",
    "\n",
    "# 2. Trip IDs are unique\n",
    "total = q(\"SELECT count(*) as n FROM main_staging.stg_yellow_trips\")['n'][0]\n",
    "distinct = q(\"SELECT count(DISTINCT trip_id) as n FROM main_staging.stg_yellow_trips\")['n'][0]\n",
    "print(f\"Total rows: {total:,}, Distinct trip_ids: {distinct:,}, Duplicates: {total - distinct}\")\n",
    "\n",
    "# 3. All trip durations are positive (after intermediate filtering)\n",
    "neg_dur = q(\"SELECT count(*) as n FROM main_intermediate.int_trip_metrics WHERE trip_duration_minutes < 0\")['n'][0]\n",
    "print(f\"Negative durations: {neg_dur} (expected: 0)\")\n",
    "\n",
    "# 4. All fares are non-negative\n",
    "neg_fares = q(\"SELECT count(*) as n FROM main_staging.stg_yellow_trips WHERE fare_amount < 0\")['n'][0]\n",
    "print(f\"Negative fares in staging: {neg_fares} (expected: 0 -- filtered in staging)\")\n",
    "\n",
    "# 5. Mart contracts -- check column types\n",
    "contracts = q(\"\"\"\n",
    "    SELECT table_name, count(*) as columns\n",
    "    FROM information_schema.columns\n",
    "    WHERE table_schema = 'main_marts'\n",
    "    GROUP BY table_name\n",
    "    ORDER BY table_name\n",
    "\"\"\")\n",
    "print(f\"\\nMart models with enforced contracts:\")\n",
    "for _, row in contracts.iterrows():\n",
    "    print(f\"  {row['table_name']}: {row['columns']} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit Tests: Testing Logic Without Real Data\n",
    "\n",
    "Unit tests are powerful because they test **transformation logic in isolation**. You provide mock inputs and assert expected outputs.\n",
    "\n",
    "Example from our project -- testing that the staging model renames columns correctly:\n",
    "\n",
    "```yaml\n",
    "# _unit_tests.yml (staging)\n",
    "unit_tests:\n",
    "  - name: test_stg_trips_renames_and_casts\n",
    "    model: stg_yellow_trips\n",
    "    given:\n",
    "      - input: source('raw_nyc_taxi', 'raw_yellow_trips')\n",
    "        format: sql  # needed for dbt-duckdb external sources\n",
    "        rows: |\n",
    "          SELECT 2 AS \"VendorID\", 161 AS \"PULocationID\", ...\n",
    "    expect:\n",
    "      rows:\n",
    "        - {vendor_id: 2, pickup_location_id: 161, ...}\n",
    "```\n",
    "\n",
    "Key insight: For dbt-duckdb external sources, you must use `format: sql` for mock inputs because the source doesn't exist as a physical table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Advanced Features\n",
    "\n",
    "### Incremental Models: Only Process New Data\n",
    "\n",
    "`fct_trips` is materialized as `incremental` with `delete+insert` strategy. On subsequent runs, it only processes trips newer than what's already in the table.\n",
    "\n",
    "```sql\n",
    "-- fct_trips.sql (key part)\n",
    "{{ config(\n",
    "    materialized='incremental',\n",
    "    unique_key='trip_id',\n",
    "    incremental_strategy='delete+insert',\n",
    "    on_schema_change='fail'\n",
    ") }}\n",
    "\n",
    "...\n",
    "{% if is_incremental() %}\n",
    "where t.pickup_datetime > (select max(pickup_datetime) from {{ this }})\n",
    "{% endif %}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fct_trips materialization type from dbt\n",
    "fct_count = q(\"SELECT count(*) as n FROM main_marts.fct_trips\")['n'][0]\n",
    "print(f\"fct_trips: {fct_count:,} rows\")\n",
    "print(\"\\nOn a full refresh: processes all ~2.9M rows (~4 seconds)\")\n",
    "print(\"On incremental run: processes 0 new rows (~0.2 seconds)\")\n",
    "print(\"\\nThis is the difference between 'rebuild everything' and 'process only changes'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Contracts: Enforce Schema at Build Time\n",
    "\n",
    "All 7 mart models have **contracts enforced**. This means dbt checks the output schema (column names and data types) at build time. If a model produces a column with the wrong type, the build fails.\n",
    "\n",
    "```yaml\n",
    "# core.yml (excerpt)\n",
    "- name: fct_trips\n",
    "  config:\n",
    "    contract:\n",
    "      enforced: true\n",
    "  columns:\n",
    "    - name: trip_id\n",
    "      data_type: varchar\n",
    "    - name: fare_amount\n",
    "      data_type: decimal(10,2)\n",
    "```\n",
    "\n",
    "This catches bugs early -- if someone changes a column type upstream, the build breaks with a clear error instead of silently corrupting data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Models: Anomaly Detection\n",
    "\n",
    "`anomaly_daily_trips` is a native **dbt Python model** that uses pandas for statistical analysis. It applies two anomaly detection methods:\n",
    "\n",
    "1. **Z-score**: Flag days where trips/revenue are >2 standard deviations from the mean\n",
    "2. **IQR (Interquartile Range)**: Flag days outside 1.5x the IQR -- more robust to outliers\n",
    "\n",
    "This demonstrates when to reach for Python over SQL: statistical functions, ML pipelines, or complex transformations that are awkward in pure SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anomaly detection results\n",
    "anomalies = q(\"\"\"\n",
    "    SELECT\n",
    "        pickup_date,\n",
    "        pickup_day_of_week,\n",
    "        is_weekend,\n",
    "        total_trips,\n",
    "        total_revenue,\n",
    "        total_trips_z_score,\n",
    "        total_revenue_z_score,\n",
    "        is_anomaly\n",
    "    FROM main_marts.anomaly_daily_trips\n",
    "    ORDER BY total_trips_z_score ASC\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Total days analyzed: {len(anomalies)}\")\n",
    "print(f\"Anomalous days: {anomalies['is_anomaly'].sum()}\")\n",
    "\n",
    "print(\"\\n=== Flagged Anomalies ===\")\n",
    "display(anomalies[anomalies['is_anomaly'] == True])\n",
    "\n",
    "print(\"\\n=== Most Extreme Z-Scores (top 5 each direction) ===\")\n",
    "display(anomalies.head(5)[['pickup_date', 'pickup_day_of_week', 'total_trips', 'total_trips_z_score']])\n",
    "display(anomalies.tail(5)[['pickup_date', 'pickup_day_of_week', 'total_trips', 'total_trips_z_score']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### External Materialization: Parquet Export\n",
    "\n",
    "`export_daily_revenue` uses dbt-duckdb's `external` materialization to write data directly to a parquet file. This is useful when you need to share data with tools outside DuckDB (Jupyter, Polars, Spark, S3 uploads)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "parquet_path = \"exports/daily_revenue.parquet\"\n",
    "if os.path.exists(parquet_path):\n",
    "    size_kb = os.path.getsize(parquet_path) / 1024\n",
    "    # Read directly with DuckDB -- no need for the dbt-managed database\n",
    "    exported = duckdb.query(f\"SELECT * FROM read_parquet('{parquet_path}') LIMIT 5\").fetchdf()\n",
    "    rows = duckdb.query(f\"SELECT count(*) FROM read_parquet('{parquet_path}')\").fetchone()[0]\n",
    "    print(f\"Exported parquet: {parquet_path} ({size_kb:.1f} KB, {rows} rows)\")\n",
    "    print(\"\\nThis file can be read by any tool that supports parquet:\")\n",
    "    print(\"  - pandas: pd.read_parquet('exports/daily_revenue.parquet')\")\n",
    "    print(\"  - polars: pl.read_parquet('exports/daily_revenue.parquet')\")\n",
    "    print(\"  - DuckDB: SELECT * FROM read_parquet('exports/daily_revenue.parquet')\")\n",
    "    display(exported)\n",
    "else:\n",
    "    print(f\"File not found: {parquet_path}\")\n",
    "    print(\"Run: cd nyc_taxi_dbt && uv run dbt run --select export_daily_revenue --profiles-dir .\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Snapshots -- Tracking Changes Over Time\n",
    "\n",
    "`snap_locations` implements **SCD Type 2** (Slowly Changing Dimensions) for taxi zone definitions. If a zone's borough or name changes, the old record gets a `dbt_valid_to` timestamp and a new record is inserted.\n",
    "\n",
    "In practice, taxi zones rarely change -- but the pattern is essential for production systems tracking things like product prices, customer addresses, or employee roles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snapshot: SCD Type 2 columns\n",
    "snap = q(\"\"\"\n",
    "    SELECT\n",
    "        location_id,\n",
    "        borough,\n",
    "        zone_name,\n",
    "        dbt_valid_from,\n",
    "        dbt_valid_to,\n",
    "        dbt_scd_id\n",
    "    FROM snapshots.snap_locations\n",
    "    ORDER BY location_id\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "total_snap = q(\"SELECT count(*) as n FROM snapshots.snap_locations\")['n'][0]\n",
    "current = q(\"SELECT count(*) as n FROM snapshots.snap_locations WHERE dbt_valid_to IS NULL\")['n'][0]\n",
    "historical = total_snap - current\n",
    "\n",
    "print(f\"Snapshot records: {total_snap} total ({current} current, {historical} historical)\")\n",
    "display(snap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Putting It All Together -- The Full DAG\n",
    "\n",
    "Here's how everything connects:\n",
    "\n",
    "```\n",
    "┌─────────────┐     ┌──────────────────┐     ┌──────────────────┐     ┌─────────────────────────┐\n",
    "│  SOURCES    │     │    STAGING       │     │  INTERMEDIATE    │     │        MARTS            │\n",
    "│             │     │                  │     │                  │     │                         │\n",
    "│ parquet ────┼────>│ stg_yellow_trips ┼────>│ int_trip_metrics ┼────>│ fct_trips (incremental) │\n",
    "│             │     │                  │     │     │            │     │   │                     │\n",
    "│ seeds:      │     │ stg_taxi_zones ──┼─────┼─────┼────────────┼────>│ dim_locations           │\n",
    "│ zone_lookup │     │ stg_payments ────┼─────┼─────┼────────────┼────>│ dim_payment_types       │\n",
    "│ payment_lkp │     │ stg_rate_codes   │     │     │            │     │ dim_dates               │\n",
    "│ rate_lkp    │     │                  │     │     v            │     │                         │\n",
    "│             │     │                  │     │ int_daily_summary┼────>│ mart_daily_revenue      │\n",
    "│             │     │                  │     │     │            │     │   └─> export (parquet)  │\n",
    "│             │     │                  │     │     └────────────┼────>│ anomaly_daily (Python)  │\n",
    "│             │     │                  │     │ int_hourly_patt. ┼────>│ mart_hourly_demand      │\n",
    "│             │     │                  │     │                  │     │                         │\n",
    "│             │     │                  │     │                  │     │ mart_location_perf      │\n",
    "└─────────────┘     └──────────────────┘     └──────────────────┘     └─────────────────────────┘\n",
    "```\n",
    "\n",
    "**18 models. 91 tests. 1 snapshot. 2 exposures. Zero infrastructure.**\n",
    "\n",
    "That's the power of dbt + DuckDB: a complete, tested, documented analytics pipeline running entirely on your laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary: what did we build?\n",
    "print(\"=\" * 60)\n",
    "print(\"  PROJECT SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "tables = q(\"\"\"\n",
    "    SELECT table_schema, table_name, table_type\n",
    "    FROM information_schema.tables\n",
    "    WHERE table_schema NOT IN ('information_schema', 'pg_catalog')\n",
    "    ORDER BY table_schema, table_name\n",
    "\"\"\")\n",
    "\n",
    "for schema in tables['table_schema'].unique():\n",
    "    schema_tables = tables[tables['table_schema'] == schema]\n",
    "    print(f\"\\n  {schema}:\")\n",
    "    for _, row in schema_tables.iterrows():\n",
    "        count = q(f\"SELECT count(*) as n FROM {schema}.{row['table_name']}\")['n'][0]\n",
    "        print(f\"    {row['table_name']:<35} {count:>12,} rows  ({row['table_type']})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "total_tests = 91\n",
    "print(f\"  Tests: {total_tests} (84 data + 7 unit)\")\n",
    "print(f\"  Materializations: view, table, incremental, external, python\")\n",
    "print(f\"  Advanced: contracts, snapshots, exposures, source freshness\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "con.close()\n",
    "print(\"Connection closed. Happy learning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Now that you understand the full pipeline, try these exercises:\n",
    "\n",
    "1. **Add a new dimension**: Create `dim_rate_codes` from `stg_rate_codes` and join it into `fct_trips`\n",
    "2. **Create a new mart**: `mart_borough_comparison` comparing metrics across boroughs\n",
    "3. **Write a unit test**: Test that `int_daily_summary` correctly counts credit card vs cash trips\n",
    "4. **Try the CLI tools**:\n",
    "   ```bash\n",
    "   make shell       # Explore the database interactively\n",
    "   make validate    # Run the validation suite\n",
    "   make benchmark   # See performance numbers\n",
    "   make docs        # Browse the auto-generated documentation site\n",
    "   ```\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [dbt Documentation](https://docs.getdbt.com/)\n",
    "- [dbt Best Practices](https://docs.getdbt.com/best-practices/how-we-structure/1-guide-overview)\n",
    "- [DuckDB Documentation](https://duckdb.org/docs/)\n",
    "- [NYC TLC Trip Data](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
